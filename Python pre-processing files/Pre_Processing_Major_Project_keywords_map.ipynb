{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pre_Processing_Major_Project_keywords_map.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0EDToQWe-pnM",
        "outputId": "a21ad361-a083-4446-de90-291cf7dac844"
      },
      "source": [
        "%%javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}setInterval(ClickConnect,60000)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}setInterval(ClickConnect,60000)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLEphHlUEUfg"
      },
      "source": [
        "# https://www.kaggle.com/devashishtiwari265/keyword-extraction-using-tf-idf/notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvRMqfxt-pnQ",
        "outputId": "f44c18a2-9a65-4111-dd75-d385ae6280a0"
      },
      "source": [
        "!mkdir -p dataset\n",
        "%cd dataset\n",
        "!wget \"https://raw.githubusercontent.com/darkshadow013/Major-Project/master/dataset/papers.csv\"\n",
        "%cd .."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/dataset\n",
            "--2021-04-28 07:37:59--  https://raw.githubusercontent.com/darkshadow013/Major-Project/master/dataset/papers.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9850371 (9.4M) [text/plain]\n",
            "Saving to: ‘papers.csv’\n",
            "\n",
            "papers.csv          100%[===================>]   9.39M  50.0MB/s    in 0.2s    \n",
            "\n",
            "2021-04-28 07:38:00 (50.0 MB/s) - ‘papers.csv’ saved [9850371/9850371]\n",
            "\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BlPUojI-KdU"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "os.system(\"python3 -m nltk.downloader wordnet\")\n",
        "os.system(\"python3 -m nltk.downloader stopwords\")\n",
        "os.system(\"mkdir -p ProcessingData\")\n",
        "\n",
        "df = pd.read_csv('dataset/papers.csv')\n",
        "df = df.head(500)\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "extra_words = [\"fig\", \"show\", \"result\", \"large\", \"also\", \"one\", \"two\", \"three\", \"four\", \"five\", \"seven\",\"eight\",\"nine\", \"figure\",\"image\",\"sample\",\"using\"]\n",
        "stop_words = list(stop_words.union(extra_words))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jyxK4MZ_EIz"
      },
      "source": [
        "def pre_process_lemmatize(text):\n",
        "    text=text.lower()\n",
        "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "    text = text.split()\n",
        "    text = [word for word in text if word not in stop_words]\n",
        "    text = [word for word in text if len(word) >= 3]\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    text = [lmtzr.lemmatize(word) for word in text]\n",
        "    return ' '.join(text)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTB0wCa6_bR5"
      },
      "source": [
        "if not os.path.exists(\"ProcessingData/docs.pkl\"):\n",
        "  docs = df['paper_text'].apply(lambda x:pre_process_lemmatize(x))\n",
        "  docs.to_pickle(\"ProcessingData/docs.pkl\")\n",
        "else:\n",
        "  docs = pd.read_pickle(\"ProcessingData/docs.pkl\")\n",
        "  print(len(docs))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ty-V2aYXGzk"
      },
      "source": [
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv=CountVectorizer(max_df=0.95,max_features=10000,ngram_range=(1,3))\n",
        "if not os.path.exists(\"ProcessingData/cv.pkl\"):\n",
        "  cv.fit(docs)\n",
        "  pickle.dump(cv, open(\"ProcessingData/cv.pkl\", 'wb'))\n",
        "else:\n",
        "  cv = pickle.load(open(\"ProcessingData/cv.pkl\", 'rb'))\n",
        "  print(cv)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAiLh9nEMJUx"
      },
      "source": [
        "if not os.path.exists(\"ProcessingData/word_count_vector.pkl\"):\n",
        "  word_count_vector=cv.transform(docs)\n",
        "  pickle.dump(word_count_vector, open(\"ProcessingData/word_count_vector.pkl\", 'wb'))\n",
        "else:\n",
        "  word_count_vector = pickle.load(open(\"ProcessingData/word_count_vector.pkl\", 'rb'))\n",
        "  print(word_count_vector)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0DE5yLkAiBJ"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "\n",
        "if not os.path.exists(\"ProcessingData/tfidf_transformer.pkl\"):\n",
        "  tfidf_transformer.fit(word_count_vector)\n",
        "  pickle.dump(tfidf_transformer, open(\"ProcessingData/tfidf_transformer.pkl\", 'wb'))\n",
        "else:\n",
        "  tfidf_transformer = pickle.load(open(\"ProcessingData/tfidf_transformer.pkl\", 'rb'))\n",
        "  print(tfidf_transformer)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExEu4CCI_nrv"
      },
      "source": [
        "def sort_coo_matrix(input_coo_matrix):\n",
        "    tuples = zip(input_coo_matrix.col, input_coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "    sorted_items = sorted_items[:topn]\n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "\n",
        "    for idx, score in sorted_items:\n",
        "        fname = feature_names[idx]\n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "    \n",
        "    return results\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhRTk7XCKy7P"
      },
      "source": [
        "feature_names=cv.get_feature_names()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6v3McBOMYtt"
      },
      "source": [
        "def get_keywords(idx, docs):\n",
        "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs[idx]]))\n",
        "    sorted_items=sort_coo_matrix(tf_idf_vector.tocoo())\n",
        "    keywords=extract_topn_from_vector(feature_names,sorted_items,5)\n",
        "    \n",
        "    return keywords\n",
        "\n",
        "def print_results(idx,keywords, df):\n",
        "    # now print the results\n",
        "    print(\"\\n=====Title=====\")\n",
        "    print(df['title'][idx])\n",
        "    #print(\"\\n=====Abstract=====\")\n",
        "    #print(df['abstract'][idx])\n",
        "    print(\"\\n===Keywords===\")\n",
        "    for k in keywords:\n",
        "        print(k)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWbCoWbRG-9G"
      },
      "source": [
        "keywordsMap = dict()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnNJcEBDMf4b"
      },
      "source": [
        "for idx in range(0,len(docs)):\n",
        "  keywords=get_keywords(idx, docs)\n",
        "  #print_results(idx,keywords, df)\n",
        "  title = df['title'][idx]\n",
        "  for k in keywords:\n",
        "    if k not in keywordsMap:\n",
        "      keywordsMap[k] = []\n",
        "    keywordsMap[k].append(title)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVgWti_dOVh3"
      },
      "source": [
        "import json\n",
        "with open(\"keywordsMap.json\", \"w\") as outfile:  \n",
        "    json.dump(keywordsMap, outfile, indent = 4) "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DNh0C81O0ox"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}